#!/bin/bash
# OpenClaw Local AI Setup Script
# One-command installation for macOS and Linux

set -e

echo "âš¡ Local AI Setup Script"
echo "========================"

# Check OS
if [[ "$OSTYPE" == "darwin"* ]]; then
    OS="macos"
    echo "âœ“ macOS detected"
elif [[ "$OSTYPE" == "linux-gnu"* ]]; then
    OS="linux"
    echo "âœ“ Linux detected"
else
    echo "âœ— Unsupported OS: $OSTYPE"
    exit 1
fi

# Check for Homebrew (macOS)
if [ "$OS" == "macos" ]; then
    if ! command -v brew &> /dev/null; then
        echo "Installing Homebrew..."
        /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
    fi
fi

# Install dependencies
echo ""
echo "ðŸ“¦ Installing dependencies..."

if [ "$OS" == "macos" ]; then
    brew install python@3.11 node git
elif [ "$OS" == "linux" ]; then
    sudo apt-get update
    sudo apt-get install -y python3.11 python3-pip nodejs npm git
fi

# Install Ollama (easiest way to run models)
echo ""
echo "ðŸ¦™ Installing Ollama..."
if ! command -v ollama &> /dev/null; then
    curl -fsSL https://ollama.com/install.sh | sh
    echo "âœ“ Ollama installed"
else
    echo "âœ“ Ollama already installed"
fi

# Pull recommended models
echo ""
echo "ðŸ“¥ Downloading starter models..."
echo "This may take 10-30 minutes depending on your connection..."

ollama pull llama3.1:8b
echo "âœ“ Llama 3.1 8B (general purpose)"

ollama pull codellama:7b
echo "âœ“ CodeLlama 7B (coding)"

ollama pull nomic-embed-text
echo "âœ“ Nomic Embed (embeddings)"

# Install Python packages
echo ""
echo "ðŸ Installing Python packages..."
pip3 install --user openai requests

# Create workspace
echo ""
echo "ðŸ“ Setting up workspace..."
mkdir -p ~/local-ai/models
mkdir -p ~/local-ai/projects
mkdir -p ~/local-ai/prompts

# Create test script
cat > ~/local-ai/test.py << 'EOF'
#!/usr/bin/env python3
"""Quick test of local AI setup"""

import subprocess
import json

def test_ollama():
    """Test if Ollama is running"""
    try:
        result = subprocess.run(
            ['ollama', 'list'],
            capture_output=True,
            text=True,
            timeout=10
        )
        if result.returncode == 0:
            print("âœ“ Ollama is running")
            print("\nInstalled models:")
            print(result.stdout)
            return True
    except Exception as e:
        print(f"âœ— Ollama test failed: {e}")
        return False

def test_model():
    """Test model inference"""
    print("\nðŸ§ª Testing model inference...")
    try:
        result = subprocess.run(
            ['ollama', 'run', 'llama3.1:8b', 'Say "Local AI is working!" and nothing else.'],
            capture_output=True,
            text=True,
            timeout=30
        )
        if result.returncode == 0:
            print("âœ“ Model responded:")
            print(result.stdout)
            return True
    except Exception as e:
        print(f"âœ— Model test failed: {e}")
        return False

if __name__ == "__main__":
    print("ðŸš€ Testing Local AI Setup")
    print("=" * 40)
    
    if test_ollama() and test_model():
        print("\nâœ… All tests passed!")
        print("\nNext steps:")
        print("1. Try: ollama run llama3.1:8b")
        print("2. Read the full guide in the blueprint")
        print("3. Join our Discord for help")
    else:
        print("\nâŒ Some tests failed. Check the troubleshooting guide.")
EOF

chmod +x ~/local-ai/test.py

# Create startup script
cat > ~/local-ai/start.sh << 'EOF'
#!/bin/bash
# Start local AI services

echo "ðŸš€ Starting Local AI..."

# Check if Ollama is running
if ! pgrep -x "ollama" > /dev/null; then
    echo "Starting Ollama..."
    ollama serve &
    sleep 2
fi

echo "âœ“ Local AI is ready!"
echo ""
echo "Try these commands:"
echo "  ollama list          # See installed models"
echo "  ollama run llama3.1  # Chat with AI"
echo "  python3 ~/local-ai/test.py  # Run tests"
EOF

chmod +x ~/local-ai/start.sh

# Final message
echo ""
echo "âœ… Setup complete!"
echo ""
echo "ðŸ“‹ Quick Start:"
echo "1. Run: ~/local-ai/start.sh"
echo "2. Test: python3 ~/local-ai/test.py"
echo "3. Chat: ollama run llama3.1:8b"
echo ""
echo "ðŸ“š Next: Read the full blueprint guide"
echo "ðŸ’¬ Join: Discord community for support"
